num_train_epochs: 10
per_device_train_batch_size: 1
per_device_eval_batch_size: 1
warmup_steps: 0
weight_decay: 0.01
logging_dir: "./logs"
logging_steps: 10
evaluation_strategy: "steps"
eval_steps: 500
save_steps: 1000
gradient_accumulation_steps: 32
optim: "adamw_torch"  # Memory-efficient optimizer
fp16: False
learning_rate: 2e-5